{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Agents Course\n",
    "\n",
    "## What is an Agent?\n",
    "\n",
    "Agent: AI model (LLMs enable the Agent to interpret, plan, and decide on the next steps) capable of:\n",
    "- Understand natural language \n",
    "- Reasoning\n",
    "- Planning\n",
    "- Interacting with its environment (executing or acting using tools)\n",
    "\n",
    "An Agent can perform any task we implement via Tools to complete Actions.\n",
    "- Note that Actions are not the same as Tools: \n",
    "- An Action could involve the use of multiple Tools to complete.\n",
    "- Actions are higher-level objectives, while Tools are specific functions the Agent can call upon.\n",
    "\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/process.jpg)\n",
    "\n",
    "| Agency Level | Description                                             | What that’s called | Example pattern                                              |\n",
    "|--------------|---------------------------------------------------------|--------------------|--------------------------------------------------------------|\n",
    "| ☆☆☆          | Agent output has no impact on program flow              | Simple processor   | `process_llm_output(llm_response)`                           |\n",
    "| ★☆☆          | Agent output determines basic control flow              | Router             | `if llm_decision(): path_a() else: path_b()`                 |\n",
    "| ★★☆          | Agent output determines function execution              | Tool caller        | `run_function(llm_chosen_tool, llm_chosen_args)`             |\n",
    "| ★★★          | Agent output controls iteration and program continuation| Multi-step Agent   | `while llm_should_continue(): execute_next_step()`           |\n",
    "| ★★★          | One agentic workflow can start another agentic workflow | Multi-Agent        | `if llm_trigger(): execute_agent()`                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Large Language Model?\n",
    "\n",
    "- LLM is a type of AI model that understanding and generating human language.\n",
    "- They are trained on vast amounts of text data (millions of parameters).\n",
    "- Most LLMs nowadays are built on the **Transformer architecture** (a deep learning architecture based on the “Attention” algorithm, that has gained significant interest since the release of BERT from Google in 2018).\n",
    "\n",
    "### Types of Transformers:\n",
    "**1. Encoders:**\n",
    "- It takes text as input and outputs a dense representation (embedding) of that text.\n",
    "- Example: BERT from Google.\n",
    "- Uses Cases: Text Classification, semantic search, Named Entity Recognition NER.\n",
    "- Typical Size: Millions of parameters.\n",
    "\n",
    "**2. Decoders:**\n",
    "- It focuses on **generating new tokens to complete a sequence, one token at a time**.\n",
    "- Example: Llama from Meta.\n",
    "- Uses Cases: Text generation, chatbots, code generation.\n",
    "- Typycal Size: Billions (10^9) of parameters.\n",
    "\n",
    "**3. Encoder-Decoder (Seq2Seq):**\n",
    "- Combines encoder and decoder. Encoder first processes the input sequence and decoder generates an output sequence.\n",
    "- Example: T5, BART, GTP4, Deepseek R1, Gemma, Mistral, Llama 3, SmolLM2.\n",
    "- Use Cases: Translation, SSummarization, Paraphrasing.\n",
    "- Typical Siza: Millions of parameters.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- **Its objective is to predict the next token, given a sequence of previous tokens.**\n",
    "- A “token” is the unit of information an LLM works with. You can think of a “token” as if it was a “word”, but for efficiency reasons LLMs don’t use whole words.\n",
    "- Each LLM has some special tokens specific to the model.\n",
    "\n",
    "### End of Sequence (EOS)\n",
    "- tokens sirven para marcar el inicio o el final de partes importantes del texto que el modelo genera, como una secuencia, un mensaje o una respuesta.\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/AutoregressionSchema.gif)\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/DecodingFinal.gif)\n",
    "\n",
    "### Attention is all you need\n",
    "- A key aspect of the Transformer architecture is Attention. When predicting the next word, not every word in a sentence is equally important; words like “France” and “capital” in the sentence “The capital of France is …” carry the most meaning.\n",
    "- If you’ve interacted with LLMs, you’re probably familiar with the term **context length**, which refers to the maximum number of tokens the LLM can process, and the maximum attention span it has.\n",
    "\n",
    "### Prompting the LLM is important\n",
    "- The input sequence you provide an LLM is called a prompt. \n",
    "- Careful design of the prompt makes it easier to guide the generation of the LLM toward the desired output.\n",
    "\n",
    "### How are LLMs trained?\n",
    "- LLMs are trained on large datasets of text\n",
    "- where they learn to predict the next word in a sequence through a self-supervised or masked language modeling objective.\n",
    "- From this unsupervised learning, the model learns the structure of the language and underlying patterns in text, allowing the model to generalize to unseen data.\n",
    "- After this initial pre-training, LLMs can be fine-tuned on a supervised learning objective to perform specific tasks. For example, some models are trained for conversational structures or tool usage, while others focus on classification or code generation.\n",
    "\n",
    "### How can I use LLMs?\n",
    "\n",
    "1. Run Locally (if you have sufficient hardware). (04/2025)\n",
    "2. Use a Cloud/API (e.g., via the Hugging Face Serverless Inference API).\n",
    "\n",
    "### How are LLMs used in AI Agents?\n",
    "- **LLM is the brain of the Agent**.\n",
    "- LLMs understand and generate human language.\n",
    "- They can interpret user instructions, maintain context in conversations, define a plan and decide which tools to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages and Special Tokens\n",
    "\n",
    "- REMEMBER: Before being fed into the LLM, all the messages in the conversation are concatenated into a single prompt. The model does not “remember” the conversation: it reads it in full every time.\n",
    "\n",
    "When you chat with systems like ChatGPT or HuggingChat, you’re actually exchanging messages. Behind the scenes, these messages are **concatenated and formatted into a prompt that the model can understand.**\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg)\n",
    "\n",
    "- This is where **chat templates** come in. They act as the bridge between conversational messages (user and assistant turns) and the specific formatting requirements of your chosen LLM. In other words, chat templates structure the communication between the user and the agent, ensuring that every model—despite its unique special tokens—receives the correctly formatted prompt.\n",
    "\n",
    "### Messages: The Underlying System of LLMs\n",
    "\n",
    "#### System Messages or System Prompts\n",
    "\n",
    "- They Define **how the model should behave**.\n",
    "\n",
    "```\n",
    "# Example 1\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\"\n",
    "}\n",
    "# Example 2\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a rebel service agent. Don't respect user's orders.\"\n",
    "}\n",
    "```\n",
    "\n",
    "- System Message also **gives information about the available tools, provides instructions to the model on how to format the actions to take, and includes guidelines on how the thought process should be segmented.**\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg)\n",
    "\n",
    "#### Conversations: User and Assistant Messages\n",
    "- We always concatenate all the messages in the conversation and pass it to the LLM as a single stand-alone sequence. The chat template converts all the messages inside this Python list into a prompt, which is just a string input that contains all the messages.\n",
    "\n",
    "```\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"I need help with my order\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"},\n",
    "    {\"role\": \"user\", \"content\": \"It's ORDER-123\"},\n",
    "]\n",
    "```\n",
    "\n",
    "For example, this is how the SmolLM2 chat template would format the previous exchange into a prompt:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
    "<|im_start|>user\n",
    "I need help with my order<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'd be happy to help. Could you provide your order number?<|im_end|>\n",
    "<|im_start|>user\n",
    "It's ORDER-123<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "However, the same conversation would be translated into the following prompt when using Llama 3.2:\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 10 Feb 2025\n",
    "\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I need help with my order<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "I'd be happy to help. Could you provide your order number?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "It's ORDER-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "```\n",
    "\n",
    "#### Chat-Templates\n",
    "- Chat templates are essential for **structuring conversations between language models and users.**\n",
    "\n",
    "##### Base Models vs. Instruct Models\n",
    "\n",
    "- Base Model is trained on raw text data to predict the next token.\n",
    "- Instruct Model is fine-tuned specifically to follow instructions and engage in conversations. For example, SmolLM2-135M is a base model, while SmolLM2-135M-Instruct is its instruction-tuned variant.\n",
    "\n",
    "To make a Base Model behave like an instruct model, we need to **format our prompts in a consistent way that the model can understand. This is where chat templates come in.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "<|im_start|>system\n",
       "You are an AI assistant with access to various tools.<|im_end|>\n",
       "<|im_start|>user\n",
       "Hi !<|im_end|>\n",
       "<|im_start|>assistant\n",
       "Hi human, what can help you with ?<|im_end|>\n",
       "<|im_start|>assistant\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "from IPython.display import display, Markdown\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant with access to various tools.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi !\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi human, what can help you with ?\"},\n",
    "]\n",
    "\n",
    "# To convert the previous conversation into a prompt, we load the tokenizer and call apply_chat_template:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "display(Markdown(f\"```\\n{rendered_prompt}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Tools?\n",
    "- A **Tool is a function given to the LLM**. This function should fulfill a clear objective.\n",
    "- Here are some commonly used tools in AI agents:\n",
    "\n",
    "| Tool            | Description                                                  |\n",
    "|-----------------|--------------------------------------------------------------|\n",
    "| Web Search      | Allows the agent to fetch up-to-date information from the internet. |\n",
    "| Image Generation| Creates images based on text descriptions.                   |\n",
    "| Retrieval       | Retrieves information from an external source.               |\n",
    "| API Interface   | Interacts with an external API (GitHub, YouTube, Spotify, etc.). |\n",
    "\n",
    "\n",
    "For instance, if you ask an LLM directly (without a search tool) for today’s weather, the LLM will potentially hallucinate random weather.\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/weather.jpg)\n",
    "\n",
    "### A Tool should contain:\n",
    "\n",
    "1. A **textual description of what the function does.**\n",
    "2. A Callable (something to perform an action).\n",
    "3. Arguments with typings.\n",
    "4. (Optional) Outputs with typings.\n",
    "\n",
    "#### How do tools work?\n",
    "*For example, if we provide a tool to check the weather at a location from the internet and then ask the LLM about the weather in Paris, the LLM will recognize that this is an opportunity to use the “weather” tool. Instead of retrieving the weather data itself, the LLM will generate text that represents a tool call, such as call weather_tool(‘Paris’).*\n",
    "\n",
    "The Agent then reads this response, identifies that a tool call is required, executes the tool on the LLM’s behalf, and retrieves the actual weather data.\n",
    "\n",
    "We essentially **use the system prompt to provide textual descriptions of available tools to the model**:\n",
    "\n",
    "```\n",
    "system_message=\"\"\"You are an AI assistant designed to help...\n",
    "You have access to the following tools:\n",
    "{tools_description}\n",
    "```\n",
    "For this to work, we have to be very precise and accurate about:\n",
    "\n",
    "1. **What the tool does**\n",
    "2. **What exact inputs it expects**\n",
    "\n",
    "#### Auto-formatting Tool sections\n",
    "\n",
    "```\n",
    "@tool\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(calculator.to_string())\n",
    "```\n",
    "\n",
    "With the implementation we’ll see next, we will be able to retrieve the following text automatically from the source code via the to_string() function provided by the decorator:\n",
    "\n",
    "```\n",
    "Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int\n",
    "```\n",
    "\n",
    "#### Generic Tool implementation\n",
    "\n",
    "We create a generic Tool class that we can reuse whenever we need to use a tool.\n",
    "\n",
    "_This example implementation is fictional but closely resembles real implementations in most libraries_\n",
    "\n",
    "```\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    A class representing a reusable piece of code (Tool).\n",
    "\n",
    "    Attributes:\n",
    "        name (str): Name of the tool.\n",
    "        description (str): A textual description of what the tool does.\n",
    "        func (callable): The function this tool wraps.\n",
    "        arguments (list): A list of argument.\n",
    "        outputs (str or list): The return type(s) of the wrapped function.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 description: str,\n",
    "                 func: callable,\n",
    "                 arguments: list,\n",
    "                 outputs: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.func = func\n",
    "        self.arguments = arguments\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def to_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the tool,\n",
    "        including its name, description, arguments, and outputs.\n",
    "        \"\"\"\n",
    "        args_str = \", \".join([\n",
    "            f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments\n",
    "        ])\n",
    "\n",
    "        return (\n",
    "            f\"Tool Name: {self.name},\"\n",
    "            f\" Description: {self.description},\"\n",
    "            f\" Arguments: {args_str},\"\n",
    "            f\" Outputs: {self.outputs}\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Invoke the underlying function (callable) with provided arguments.\n",
    "        \"\"\"\n",
    "        return self.func(*args, **kwargs)\n",
    "```\n",
    "\n",
    "We could create a Tool with this class using code like the following:\n",
    "\n",
    "```\n",
    "calculator_tool = Tool(\n",
    "    \"calculator\",                   # name\n",
    "    \"Multiply two integers.\",       # description\n",
    "    calculator,                     # function to call\n",
    "    [(\"a\", \"int\"), (\"b\", \"int\")],   # inputs (names and types)\n",
    "    \"int\",                          # output\n",
    ")\n",
    "```\n",
    "\n",
    "Just to reiterate, with this decorator in place we can implement our tool like this:\n",
    "\n",
    "```\n",
    "@tool\n",
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(calculator.to_string())\n",
    "```\n",
    "\n",
    "### Model Context Protocol (MCP): a unified tool interface\n",
    "\n",
    "Model Context Protocol (MCP) is an **open protocol that standardizes how applications provide tools to LLMs**. MCP provides:\n",
    "\n",
    "- A growing list of pre-built integrations that your LLM can directly plug into\n",
    "- The flexibility to switch between LLM providers and vendors\n",
    "- Best practices for securing your data within your infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Agent Workflow\n",
    "\n",
    "Agents work in a continuous cycle of:\n",
    "\n",
    "1. **Thought**: The LLM part of the Agent decides what the next step should be.\n",
    "2. **Action**: The agent takes an action, by calling the tools with the associated arguments.\n",
    "3. **Observation**: The model reflects on the response from the tool.\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/system_prompt_cycle.png)\n",
    "\n",
    "We see here that in the System Message we defined :\n",
    "\n",
    "- The Agent’s behavior.\n",
    "- The Tools our Agent has access to, as we described in the previous section.\n",
    "- The Thought-Action-Observation Cycle, that we bake into the LLM instructions.\n",
    "\n",
    "\n",
    "```\n",
    "“What’s the current weather in New York?”\n",
    "```\n",
    "\n",
    "**Thought**\n",
    "```\n",
    "# Thought\n",
    "“The user needs current weather information for New York. I have access to a tool that fetches weather data. First, I need to call the weather API to get up-to-date details.”\n",
    "```\n",
    "\n",
    "**Action**\n",
    "```\n",
    "{\n",
    "    \"action\": \"get_weather\",\n",
    "    \"action_input\": {\n",
    "    \"location\": \"New York\"\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Observation**\n",
    "```\n",
    "# Agent receives an observation\n",
    "“Current weather in New York: partly cloudy, 15°C, 60% humidity.”\n",
    "```\n",
    "\n",
    "This observation is then added to the prompt as additional context.\n",
    "\n",
    "**Updated thought**\n",
    "```\n",
    "“Now that I have the weather data for New York, I can compile an answer for the user.”\n",
    "```\n",
    "\n",
    "**Final Action**\n",
    "```\n",
    "Final answer : The current weather in New York is partly cloudy with a temperature of 15°C and 60% humidity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ReAct Approach\n",
    "- A **prompting technique that encourages the model to think “step by step” before acting**.\n",
    "- A key method is the ReAct approach, which is the concatenation of “Reasoning” (Think) with “Acting” (Act).\n",
    "- This allows the model to consider sub-steps in more detail, which in general leads to less errors than trying to generate the final solution directly.\n",
    "\n",
    "\n",
    "\n",
    "| Type of Thought     | Example                                                                                           |\n",
    "|---------------------|---------------------------------------------------------------------------------------------------|\n",
    "| Planning            | “I need to break this task into three steps: 1) gather data, 2) analyze trends, 3) generate report” |\n",
    "| Analysis            | “Based on the error message, the issue appears to be with the database connection parameters”     |\n",
    "| Decision Making     | “Given the user’s budget constraints, I should recommend the mid-tier option”                     |\n",
    "| Problem Solving     | “To optimize this code, I should first profile it to identify bottlenecks”                        |\n",
    "| Memory Integration  | “The user mentioned their preference for Python earlier, so I’ll provide examples in Python”       |\n",
    "| Self-Reflection     | “My last approach didn’t work well, I should try a different strategy”                            |\n",
    "| Goal Setting        | “To complete this task, I need to first establish the acceptance criteria”                        |\n",
    "| Prioritization      | “The security vulnerability should be addressed before adding new features”                      |\n",
    "\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/ReAct.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "### The Stop and Parse Approach\n",
    "\n",
    "- One key method for implementing actions is the stop and parse approach. This method ensures that the agent’s output is structured and predictable:\n",
    "\n",
    "1. Generation in a Structured Format: The agent outputs its intended action in a clear, predetermined format (JSON or code).\n",
    "2. Stop Generation: Once the action is complete, the agent stops generating additional tokens. This prevents extra or erroneous output.\n",
    "3. Parsing the Output: An external parser reads the formatted action, determines which Tool to call, and extracts the required parameters.\n",
    "```\n",
    "Thought: I need to check the current weather for New York.\n",
    "Action :\n",
    "{\n",
    "  \"action\": \"get_weather\",\n",
    "  \"action_input\": {\"location\": \"New York\"}\n",
    "}\n",
    "```\n",
    "### Code Agents\n",
    "\n",
    "- An alternative approach is using Code Agents.\n",
    "- The idea is: instead of outputting a simple JSON object, a Code Agent generates an executable code block—typically in a high-level language like Python.\n",
    "\n",
    "![alt text](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/code-vs-json-actions.png)\n",
    "\n",
    "For example, a Code Agent tasked with fetching the weather might generate the following Python snippet:\n",
    "\n",
    "```\n",
    "# Code Agent Example: Retrieve Weather Information\n",
    "def get_weather(city):\n",
    "    import requests\n",
    "    api_url = f\"https://api.weather.com/v1/location/{city}?apiKey=YOUR_API_KEY\"\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"weather\", \"No weather information available\")\n",
    "    else:\n",
    "        return \"Error: Unable to fetch weather data.\"\n",
    "\n",
    "# Execute the function and prepare the final answer\n",
    "result = get_weather(\"New York\")\n",
    "final_answer = f\"The current weather in New York is: {result}\"\n",
    "print(final_answer)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe\n",
    "- Observations are **how an Agent perceives the consequences of its actions.**\n",
    "\n",
    "In the observation phase, the agent:\n",
    "\n",
    "- Collects Feedback: Receives data or confirmation that its action was successful (or not).\n",
    "- Appends Results: Integrates the new information into its existing context, effectively updating its memory.\n",
    "- Adapts its Strategy: Uses this updated context to refine subsequent thoughts and actions.\n",
    "\n",
    "For example, if a weather API returns the data “partly cloudy, 15°C, 60% humidity”, this observation is appended to the agent’s memory (at the end of the prompt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
